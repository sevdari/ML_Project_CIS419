{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568454, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Reviews.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393931, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop_duplicates(subset = {\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393917, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n",
    "\n",
    "import numpy as np                                  #for large and multi-dimensional arrays\n",
    "import pandas as pd\n",
    "\n",
    "import nltk                                         #Natural language processing tool-kit\n",
    "\n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "from gensim.models import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/klejdisevdari/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#basic cleaning\n",
    "import time\n",
    "\n",
    "data = data.drop_duplicates(subset = {\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "#removing spaces and stopwords\n",
    "import re\n",
    "# function to clean data\n",
    "def clean_data(X):\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "    start = time.time()\n",
    "    for i, sentence in enumerate(X):\n",
    "        if i%10000 == 0:\n",
    "            print(i, 'Time taken:', time.time()-start)\n",
    "            start = time.time()\n",
    "        sentence = sentence.lower()                 # Converting to lowercase\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n",
    "\n",
    "        words = [snow.stem(word) for word in sentence.split()]   # Stemming\n",
    "        temp.append(words)\n",
    "\n",
    "    X = temp    \n",
    "\n",
    "    sent = []\n",
    "    for row in X:\n",
    "        sequ = ''\n",
    "        for word in row:\n",
    "            sequ = sequ + ' ' + word\n",
    "        sent.append(sequ)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data['Text'], cleaned_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = cleaned_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y[y<4] = 0 # negative class\n",
    "y[y>=4] = 1 # positive class\n",
    "y.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y) # to convert to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time taken: 1.6927719116210938e-05\n",
      "10000 Time taken: 4.26750111579895\n",
      "20000 Time taken: 4.606721878051758\n",
      "30000 Time taken: 4.318329811096191\n",
      "40000 Time taken: 4.398187637329102\n",
      "50000 Time taken: 4.386837005615234\n",
      "60000 Time taken: 4.6317079067230225\n",
      "70000 Time taken: 4.903625011444092\n",
      "80000 Time taken: 4.382453918457031\n",
      "90000 Time taken: 4.531135082244873\n",
      "100000 Time taken: 4.221998929977417\n",
      "110000 Time taken: 4.624211072921753\n",
      "120000 Time taken: 4.828066110610962\n",
      "130000 Time taken: 4.346555233001709\n",
      "140000 Time taken: 4.350635051727295\n",
      "150000 Time taken: 4.977027893066406\n",
      "160000 Time taken: 4.578828811645508\n",
      "170000 Time taken: 4.368210077285767\n",
      "180000 Time taken: 4.323323965072632\n",
      "190000 Time taken: 4.393045902252197\n",
      "200000 Time taken: 4.8498170375823975\n",
      "210000 Time taken: 4.4135520458221436\n",
      "220000 Time taken: 4.37319016456604\n",
      "230000 Time taken: 4.6422529220581055\n",
      "240000 Time taken: 4.18298077583313\n",
      "250000 Time taken: 4.431384801864624\n",
      "260000 Time taken: 4.50162410736084\n",
      "270000 Time taken: 4.742058992385864\n",
      "280000 Time taken: 4.419977903366089\n",
      "290000 Time taken: 4.262532949447632\n",
      "300000 Time taken: 4.587945222854614\n",
      "310000 Time taken: 4.433168888092041\n",
      "320000 Time taken: 4.3959760665893555\n",
      "330000 Time taken: 4.2549049854278564\n",
      "340000 Time taken: 4.245659828186035\n",
      "350000 Time taken: 4.341548919677734\n",
      "360000 Time taken: 4.486767053604126\n",
      "370000 Time taken: 4.434962272644043\n",
      "380000 Time taken: 4.491758108139038\n",
      "390000 Time taken: 4.409529685974121\n"
     ]
    }
   ],
   "source": [
    "cleaned_data = clean_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393917, 393917)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data), len(y) # checking if the length of cleaned data and y is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(\"C:/Users/micha/Downloads/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20892\\2342288618.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcleaned_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cleaned_data' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393917, 1000)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_tf = cleaned_data['Text']\n",
    "tf_idf = TfidfVectorizer(max_features=1000)\n",
    "tf_data = tf_idf.fit_transform(final_tf)\n",
    "tf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf_data.toarray() # turning sparse matrix into dense matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (393917, 1000))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(temp), temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.48%\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier on training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost classifier on test data\n",
    "accuracy = xgb_clf.score(X_test, y_test)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(temp[0:2]).shape # 2 rows and 10000 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(temp, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "train_data, train_labels = torch.from_numpy(train_data).float(), torch.from_numpy(train_labels).float()\n",
    "test_data, test_labels = torch.from_numpy(test_data).float(), torch.from_numpy(test_labels).float()\n",
    "\n",
    "# Create TensorDatasets for train and test sets\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "test_dataset = TensorDataset(test_data, test_labels)\n",
    "\n",
    "# Create DataLoaders for train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(5000, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return nn.functional.sigmoid(logits)\n",
    "\n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.547\n",
      "[1,  4000] loss: 0.524\n",
      "[1,  6000] loss: 0.526\n",
      "[1,  8000] loss: 0.528\n",
      "[1, 10000] loss: 0.520\n",
      "[1, 12000] loss: 0.525\n",
      "[1, 14000] loss: 0.524\n",
      "[1, 16000] loss: 0.509\n",
      "[1, 18000] loss: 0.491\n",
      "[2,  2000] loss: 0.379\n",
      "[2,  4000] loss: 0.342\n",
      "[2,  6000] loss: 0.314\n",
      "[2,  8000] loss: 0.300\n",
      "[2, 10000] loss: 0.287\n",
      "[2, 12000] loss: 0.285\n",
      "[2, 14000] loss: 0.279\n",
      "[2, 16000] loss: 0.278\n",
      "[2, 18000] loss: 0.272\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        labels = torch.unsqueeze(labels, 1)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        try:\n",
    "            loss = criterion(outputs, labels)\n",
    "        except:\n",
    "            print(inputs, outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' i have bought sever of the vital can dog food product and have found them all to be of good qualiti the product look more like a stew than a process meat and it smell better my labrador is finicki and she appreci this product better than most'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time taken: 9.5367431640625e-07\n",
      "[' i do like this product']\n",
      "Probability of a positive review is 0.6198616623878479\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"I do like this product\"]\n",
    "clean_sentence = clean_data(sentence)\n",
    "print(clean_sentence)\n",
    "sparse = tf_idf.transform(clean_sentence)\n",
    "tensor_sentence = torch.Tensor(sparse.toarray())\n",
    "prob = net(tensor_sentence)\n",
    "print(f'Probability of a positive review is {prob.item()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training W2V Model\n",
    "sentences = [gensim.utils.simple_preprocess(text) for text in cleaned_data['Text']]\n",
    "model10 = gensim.models.Word2Vec(sentences=sentences, vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "#takes around a minute or so \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text encoding\n",
    "def encode_text(text):\n",
    "    words = gensim.utils.simple_preprocess(text)\n",
    "    vecs = [model10.wv[word] for word in words if word in model10.wv]\n",
    "    if len(vecs) > 0:\n",
    "        return np.mean(vecs, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model10.vector_size)\n",
    "\n",
    "cleaned_data['encoded_text_10'] = cleaned_data['Text'].apply(encode_text)\n",
    "\n",
    "#takes about a minute or so \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of         Unnamed: 0                                               Text  Score  \\\n",
       "0                0   i have bought sever of the vital can dog food...      1   \n",
       "1                1   product arriv label as jumbo salt peanut the ...      0   \n",
       "2                2   this is a confect that has been around a few ...      1   \n",
       "3                3   if you are look for the secret ingredi in rob...      0   \n",
       "4                4   great taffi at a great price there was a wide...      1   \n",
       "...            ...                                                ...    ...   \n",
       "393912      393912   great for sesam chicken this is a good if not...      1   \n",
       "393913      393913   im disappoint with the flavor the chocol note...      0   \n",
       "393914      393914   these star are small so you can give 10-15 of...      1   \n",
       "393915      393915   these are the best treat for train and reward...      1   \n",
       "393916      393916   i am veri satisfi product is as advertis i us...      1   \n",
       "\n",
       "                                             encoded_text  \n",
       "0       [0.47331455, -0.5940461, -1.1750864, 1.4141014...  \n",
       "1       [-0.14676346, -0.6673431, -2.3197024, 1.179353...  \n",
       "2       [0.77667487, -0.57814157, -1.4666973, 1.980593...  \n",
       "3       [0.80986726, -0.62491035, -1.0389011, 1.582608...  \n",
       "4       [-1.1170623, 0.18908764, -3.468279, 1.7661341,...  \n",
       "...                                                   ...  \n",
       "393912  [0.4723203, 1.1864258, -1.63443, 1.8057564, -0...  \n",
       "393913  [0.6762766, 0.34897706, -2.2492816, 0.8990112,...  \n",
       "393914  [0.8504282, -0.691438, -0.63638705, 1.4725218,...  \n",
       "393915  [0.5973997, -0.34821656, -1.2172682, 1.8076903...  \n",
       "393916  [0.4820882, -0.48676124, -0.6583504, 2.536344,...  \n",
       "\n",
       "[393917 rows x 4 columns]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param Value: 4 Accuracy: 0.8505449499052261\n",
      "Param Value: 5 Accuracy: 0.8539720417005144\n",
      "Param Value: 6 Accuracy: 0.8564683184402925\n",
      "Param Value: 7 Accuracy: 0.8577291497427566\n",
      "Param Value: 8 Accuracy: 0.8597515569997293\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST\n",
    "\n",
    "for param_value in [4,5,6,7,8]: \n",
    "    #Train Test Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(cleaned_data['encoded_text'], y, test_size=0.3, random_state=42)\n",
    "\n",
    "    #Model\n",
    "    xg_train = xgb.DMatrix(np.array(list(X_train)), label=y_train)\n",
    "    xg_test = xgb.DMatrix(np.array(list(X_test)), label=y_test)\n",
    "    param = {'max_depth': param_value, 'eta': 0.15, 'objective': 'binary:logistic'}\n",
    "    model = xgb.train(param, xg_train, num_boost_round=100)\n",
    "\n",
    "\n",
    "    #model eval\n",
    "    y_pred = model.predict(xg_test)\n",
    "    y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "    accuracy = np.mean(y_pred == y_test)\n",
    "    print(f'Param Value: {param_value} Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "#as this model gets more complex, we actually see it get better.... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8001455456268616\n"
     ]
    }
   ],
   "source": [
    "#model eval for 10-D W2V embeddings\n",
    "#Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(cleaned_data['encoded_text_10'], y, test_size=0.3, random_state=42)\n",
    "#Model\n",
    "xg_train = xgb.DMatrix(np.array(list(X_train)), label=y_train)\n",
    "xg_test = xgb.DMatrix(np.array(list(X_test)), label=y_test)\n",
    "param = {'max_depth': 6, 'eta': 0.3, 'objective': 'binary:logistic','lambda': 1}\n",
    "model = xgb.train(param, xg_train)\n",
    "\n",
    "y_pred = model.predict(xg_test)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "#default params, depth 6 eta 0.3, lambda = 1\n",
    "#vector size 10 \n",
    "#80% accurate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23323     1\n",
       "87124     0\n",
       "267273    1\n",
       "15891     0\n",
       "77780     1\n",
       "         ..\n",
       "288031    1\n",
       "155240    1\n",
       "372046    1\n",
       "369023    1\n",
       "349286    1\n",
       "Name: Score, Length: 118176, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# define your data and labels\n",
    "X = cleaned_data['encoded_text']\n",
    "y = y\n",
    "\n",
    "# define the parameter space to search over\n",
    "param_dist = {\n",
    "    'learning_rate': np.arange(0.05, 0.31, 0.05),\n",
    "    'max_depth': np.arange(6,10, 1),\n",
    "    'subsample': np.arange(0.5, 1.0, 0.1)\n",
    "}\n",
    "\n",
    "# create an instance of the XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# create a random search object\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # number of iterations\n",
    "    cv=5,  # number of cross-validation folds\n",
    "    scoring='roc_auc',  # evaluation metric\n",
    "    n_jobs=-1,  # use all available CPUs\n",
    "    verbose=3  # display progress messages\n",
    ")\n",
    "\n",
    "# run the random search\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# print the best parameters and score\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best ROC AUC score:\", random_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Define X and y\n",
    "X = cleaned_data['encoded_text']\n",
    "y = y\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5,6],\n",
    "    'learning_rate': [0.1, 0.01],\n",
    "    'n_estimators': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Define XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "# Define GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_clf, \n",
    "    param_grid=param_grid, \n",
    "    cv=5, \n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV object to data\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print best parameters and score\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "print('Best score:', grid_search.best_score_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shit runs out of memory oops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 7.19 TiB for an array with shape (393917, 2508978) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20892\\3976151988.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Convert sparse matrix to array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Print feature names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1297\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1298\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 7.19 TiB for an array with shape (393917, 2508978) and data type int64"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Encode documents using 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "encoded_documents = vectorizer.fit_transform(cleaned_data['Text'])\n",
    "\n",
    "\n",
    "# Define CountVectorizer with n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform text data\n",
    "X = vectorizer.fit_transform(cleaned_data['Text'])\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "X = np.array(X.toarray())\n",
    "\n",
    "# Print feature names\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8597515569997293\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST4\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Define X and y\n",
    "X = X\n",
    "y = y\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier on training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(xg_test)\n",
    "y_pred = [1 if p >= 0.5 else 0 for p in y_pred]\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for this one I don't think we can stem ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/path/to/glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20892\\3805236029.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Load GloVe embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mglove_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/path/to/glove.6B.100d.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Get embedding vector for a word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[0;32m   1627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1628\u001b[0m         \"\"\"\n\u001b[1;32m-> 1629\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1630\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1631\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[0;32m   1953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1954\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loading projection weights from %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1955\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1956\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mno_header\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1957\u001b[0m             \u001b[1;31m# deduce both vocab_size & vector_size from 1st pass over file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[0mtransport_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m     fobj = _shortcut_open(\n\u001b[0m\u001b[0;32m    189\u001b[0m         \u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'errors'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_model = KeyedVectors.load_word2vec_format('/path/to/glove.6B.100d.txt', binary=False)\n",
    "\n",
    "# Get embedding vector for a word\n",
    "embedding_vector = glove_model['word']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
