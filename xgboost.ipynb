{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")                     #Ignoring unnecessory warnings\n",
    "\n",
    "import numpy as np                                  #for large and multi-dimensional arrays\n",
    "import pandas as pd\n",
    "\n",
    "import nltk                                         #Natural language processing tool-kit\n",
    "\n",
    "from nltk.corpus import stopwords                   #Stopwords corpus\n",
    "from nltk.stem import PorterStemmer                 # Stemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer          #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer          #For TF-IDF\n",
    "from gensim.w2vs import Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basic cleaning\n",
    "import time\n",
    "\n",
    "data = data.drop_duplicates(subset = {\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "#removing spaces and stopwords\n",
    "import re\n",
    "# function to clean data\n",
    "def clean_data(X):\n",
    "    temp =[]\n",
    "    snow = nltk.stem.SnowballStemmer('english')\n",
    "    start = time.time()\n",
    "    for i, sentence in enumerate(X):\n",
    "        if i%10000 == 0:\n",
    "            print(i, 'Time taken:', time.time()-start)\n",
    "            start = time.time()\n",
    "        sentence = sentence.lower()                 # Converting to lowercase\n",
    "        cleanr = re.compile('<.*?>')\n",
    "        sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n",
    "        sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "        sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n",
    "\n",
    "        words = [snow.stem(word) for word in sentence.split()]   # Stemming\n",
    "        temp.append(words)\n",
    "\n",
    "    X = temp    \n",
    "\n",
    "    sent = []\n",
    "    for row in X:\n",
    "        sequ = ''\n",
    "        for word in row:\n",
    "            sequ = sequ + ' ' + word\n",
    "        sent.append(sequ)\n",
    "\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = pd.read_csv(\"C:/Users/micha/Downloads/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = cleaned_data['Text'], cleaned_data['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Val Test Splitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[6240, 103101, 132012, 118346, 69678, 88065, 77283, 67678, 11370, 119636, 103058, 84350, 127436, 82990, 48312, 26324, 3711, 44976, 10303, 11962, 131641, 45099, 81899, 107770, 42312, 17753, 74373, 49704, 52256, 70211, 45685, 126593, 65367, 63933, 82366, 126367, 20829, 125395, 95147, 39374, 20359, 77745, 115421, 83684, 97041, 22449, 107053, 89064, 128864, 96635, 112664, 11635, 94570, 80720, 119419, 21796, 79041, 130782, 76993, 41201, 66625, 58450, 27695, 122371, 6787, 99336, 106295, 32975, 27402, 39926, 2721, 65128, 99464, 55611, 58617, 39928, 37133, 128457, 62765, 93617, 75491, 107562, 22642, 88431, 32381, 125915, 12183, 74898, 120600, 15992, 65734, 60435, 108560, 117120, 116485, 111626, 13633, 7476, 70581, 7023, 96693, 2479, 72222, 64377, 124719, 110532, 48584, 31094, 93922, 58788, 97901, 107225, 83495, 134209, 24437, 23667, 77749, 94509, 61308, 53613, 101388, 52441, 69710, 15713, 70751, 135790, 19296, 23377, 31911, 48282, 11155, 19377, 34810, 29628, 81882, 96308, 42238, 84278, 17713, 46880, 131277, 33572, 18124, 127190, 12489, 130445, 5210, 28592, 88614, 75006, 128809, 54442, 88386, 67049, 114195, 23330, 70278, 62949, 119173, 19384, 53207, 115040, 127569, 10461, 101602, 100471, 126351, 81693, 32782, 53885, 74919, 121867, 55905, 102024, 3993, 83428, 92480, 45167, 113042, 111441, 86933, 5479, 44140, 96713, 111536, 75007, 62210, 20633, 120394, 15590, 95545, 51741, 93583, 66047, 108218, 126642, 10653, 21672, 29692, 128655, 101506, 23308, 130077, 54804, 119995, 109993, 56207, 29064, 131872, 109288, 81027, 72431, 61262, 58947, 123157, 107943, 55620, 50157, 73943, 119915, 125663, 100445, 64864, 56401, 109366, 3404, 16200, 68142, 107690, 77879, 134736, 88323, 41639, 125071, 68500, 115670, 39804, 109456, 116360, 125325, 97307, 2374, 74410, 19042, 1888, 88486, 64791, 14474, 13136, 22836, 81230, 59115, 80840, 96049, 46580, 89783, 84183, 18888, 131491, 113246, 74715, 111745, 63313, 126606, 96523, 91601, 56708, 29696, 72578, 115728, 122441, 53889, 108927, 47063, 8090, 50323, 58381, 55263, 93263, 13252, 105345, 85622, 17292, 91011, 56599, 70739, 136859, 117521, 20985, 76304, 120348, 124855, 76414, 43209, 47851, 42518, 17877, 7286, 80329, 132290, 47538, 22376, 28340, 134690, 74734, 10627, 85419, 32691, 121802, 136610, 72203, 87782, 96291, 118125, 7703, 87233, 87505, 49032, 92602, 108366, 45697, 95329, 79785, 51505, 591, 48657, 98897, 71537, 95203, 21179, 34519, 79410, 85572, 56454, 35373, 27223, 81261, 38050, 72515, 136432, 59906, 26678, 23490, 36764, 92264, 1401, 136232, 136367, 75801, 127997, 84566, 27533, 130556, 122144, 6114, 68929, 67108, 112109, 53966, 27993, 137147, 58773, 26137, 20256, 131393, 89962, 130007, 22848, 119737, 35837, 9339, 11050, 70144, 46135, 13938, 59899, 29586, 31388, 81225, 32421, 61145, 33947, 102080, 33726, 135959, 23673, 81922, 2195, 72696, 22830, 78899, 57282, 38148, 67822, 68114, 53717, 9784, 5627, 32814, 68395, 38467, 44483, 18543, 88516, 50027, 89028, 30684, 71647, 22197, 61974, 21673, 45502, 93934, 64241, 74004, 60100, 74862, 60168, 34126, 132299, 31363, 122670, 45633, 43808, 26514, 71706, 49810, 16181, 46403, 101457, 19506, 101309, 43299, 52601, 79569, 106259, 37709, 111329, 28945, 91207, 114993, 80271, 97552, 114901, 9596, 38964, 10844, 95678, 57943, 132117, 64707, 116856, 34824, 54399, 57302, 14651, 72664, 37706, 51786, 106300, 33929, 127889, 11870, 108109, 122105, 66616, 81110, 130250, 19595, 86324, 99768, 6506, 127551, 69863, 75195, 20394, 106420, 111489, 91399, 91208, 72535, 11213, 54316, 36138, 50799, 81561, 87348, 45914, 137048, 70225, 134345, 7322, 37663, 53965, 17584, 4681, 106054, 5964, 88082, 7773, 57874, 12783, 69322, 28131, 35654, 82277, 435, 42510, 20952, 106626, 22530, 102456, 126668, 122445, 117612, 115357, 6896, 47698, 77856, 47559, 14715, 32450, 53777, 96717, 124596, 132050, 40406, 83009, 18987, 38071, 76073, 137030, 44555, 47971, 1744, 134727, 121848, 14184, 71794, 10239, 36225, 114078, 127760, 93060, 40574, 13327, 107352, 49271, 9454, 78348, 3971, 14330, 69324, 83052, 129167, 97558, 14645, 97484, 70049, 46633, 87065, 93671, 20846, 10825, 9323, 9405, 40004, 119013, 114428, 12045, 85157, 92281, 31398, 77054, 53347, 126837, 122671, 76362, 49978, 97755, 27341, 85268, 10550, 117515, 108397, 73712, 7893, 70950, 32702, 15996, 91233, 830, 111734, 121951, 85437, 137808, 99613, 129486, 126453, 14242, 135650, 98973, 57520, 41957, 127303, 43683, 119353, 55573, 11174, 120117, 5815, 46554, 83167, 238, 31018, 121797, 52966, 126800, 103948, 86817, 41262, 42058, 134780, 122725, 119833, 42653, 55349, 83965, 112190, 15937, 88731, 75176, 21071, 58509, 100256, 40626, 5461, 86240, 53760, 6426, 17485, 104717, 79803, 62224, 80267, 88843, 13885, 110863, 99062, 122601, 111379] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6136\\4167040307.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_balanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_balanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_balanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val_balanced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbal_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Print the size and distribution of each set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6136\\4167040307.py\u001b[0m in \u001b[0;36mbal_split\u001b[1;34m(X, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mpositive_val_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive_val_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mnegative_val_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnegative_val_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mX_val_balanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpositive_val_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnegative_val_indices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0my_val_balanced\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 984\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1017\u001b[0m             \u001b[1;31m#  (i.e. self.iloc) or label-based (i.e. self.loc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_fallback_to_positional\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1192\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1194\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# A collection of keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1132\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1133\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[0;32m   1134\u001b[0m             \u001b[1;33m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1328\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5794\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5796\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5797\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5798\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5858\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5859\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{not_found} not in index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5861\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[6240, 103101, 132012, 118346, 69678, 88065, 77283, 67678, 11370, 119636, 103058, 84350, 127436, 82990, 48312, 26324, 3711, 44976, 10303, 11962, 131641, 45099, 81899, 107770, 42312, 17753, 74373, 49704, 52256, 70211, 45685, 126593, 65367, 63933, 82366, 126367, 20829, 125395, 95147, 39374, 20359, 77745, 115421, 83684, 97041, 22449, 107053, 89064, 128864, 96635, 112664, 11635, 94570, 80720, 119419, 21796, 79041, 130782, 76993, 41201, 66625, 58450, 27695, 122371, 6787, 99336, 106295, 32975, 27402, 39926, 2721, 65128, 99464, 55611, 58617, 39928, 37133, 128457, 62765, 93617, 75491, 107562, 22642, 88431, 32381, 125915, 12183, 74898, 120600, 15992, 65734, 60435, 108560, 117120, 116485, 111626, 13633, 7476, 70581, 7023, 96693, 2479, 72222, 64377, 124719, 110532, 48584, 31094, 93922, 58788, 97901, 107225, 83495, 134209, 24437, 23667, 77749, 94509, 61308, 53613, 101388, 52441, 69710, 15713, 70751, 135790, 19296, 23377, 31911, 48282, 11155, 19377, 34810, 29628, 81882, 96308, 42238, 84278, 17713, 46880, 131277, 33572, 18124, 127190, 12489, 130445, 5210, 28592, 88614, 75006, 128809, 54442, 88386, 67049, 114195, 23330, 70278, 62949, 119173, 19384, 53207, 115040, 127569, 10461, 101602, 100471, 126351, 81693, 32782, 53885, 74919, 121867, 55905, 102024, 3993, 83428, 92480, 45167, 113042, 111441, 86933, 5479, 44140, 96713, 111536, 75007, 62210, 20633, 120394, 15590, 95545, 51741, 93583, 66047, 108218, 126642, 10653, 21672, 29692, 128655, 101506, 23308, 130077, 54804, 119995, 109993, 56207, 29064, 131872, 109288, 81027, 72431, 61262, 58947, 123157, 107943, 55620, 50157, 73943, 119915, 125663, 100445, 64864, 56401, 109366, 3404, 16200, 68142, 107690, 77879, 134736, 88323, 41639, 125071, 68500, 115670, 39804, 109456, 116360, 125325, 97307, 2374, 74410, 19042, 1888, 88486, 64791, 14474, 13136, 22836, 81230, 59115, 80840, 96049, 46580, 89783, 84183, 18888, 131491, 113246, 74715, 111745, 63313, 126606, 96523, 91601, 56708, 29696, 72578, 115728, 122441, 53889, 108927, 47063, 8090, 50323, 58381, 55263, 93263, 13252, 105345, 85622, 17292, 91011, 56599, 70739, 136859, 117521, 20985, 76304, 120348, 124855, 76414, 43209, 47851, 42518, 17877, 7286, 80329, 132290, 47538, 22376, 28340, 134690, 74734, 10627, 85419, 32691, 121802, 136610, 72203, 87782, 96291, 118125, 7703, 87233, 87505, 49032, 92602, 108366, 45697, 95329, 79785, 51505, 591, 48657, 98897, 71537, 95203, 21179, 34519, 79410, 85572, 56454, 35373, 27223, 81261, 38050, 72515, 136432, 59906, 26678, 23490, 36764, 92264, 1401, 136232, 136367, 75801, 127997, 84566, 27533, 130556, 122144, 6114, 68929, 67108, 112109, 53966, 27993, 137147, 58773, 26137, 20256, 131393, 89962, 130007, 22848, 119737, 35837, 9339, 11050, 70144, 46135, 13938, 59899, 29586, 31388, 81225, 32421, 61145, 33947, 102080, 33726, 135959, 23673, 81922, 2195, 72696, 22830, 78899, 57282, 38148, 67822, 68114, 53717, 9784, 5627, 32814, 68395, 38467, 44483, 18543, 88516, 50027, 89028, 30684, 71647, 22197, 61974, 21673, 45502, 93934, 64241, 74004, 60100, 74862, 60168, 34126, 132299, 31363, 122670, 45633, 43808, 26514, 71706, 49810, 16181, 46403, 101457, 19506, 101309, 43299, 52601, 79569, 106259, 37709, 111329, 28945, 91207, 114993, 80271, 97552, 114901, 9596, 38964, 10844, 95678, 57943, 132117, 64707, 116856, 34824, 54399, 57302, 14651, 72664, 37706, 51786, 106300, 33929, 127889, 11870, 108109, 122105, 66616, 81110, 130250, 19595, 86324, 99768, 6506, 127551, 69863, 75195, 20394, 106420, 111489, 91399, 91208, 72535, 11213, 54316, 36138, 50799, 81561, 87348, 45914, 137048, 70225, 134345, 7322, 37663, 53965, 17584, 4681, 106054, 5964, 88082, 7773, 57874, 12783, 69322, 28131, 35654, 82277, 435, 42510, 20952, 106626, 22530, 102456, 126668, 122445, 117612, 115357, 6896, 47698, 77856, 47559, 14715, 32450, 53777, 96717, 124596, 132050, 40406, 83009, 18987, 38071, 76073, 137030, 44555, 47971, 1744, 134727, 121848, 14184, 71794, 10239, 36225, 114078, 127760, 93060, 40574, 13327, 107352, 49271, 9454, 78348, 3971, 14330, 69324, 83052, 129167, 97558, 14645, 97484, 70049, 46633, 87065, 93671, 20846, 10825, 9323, 9405, 40004, 119013, 114428, 12045, 85157, 92281, 31398, 77054, 53347, 126837, 122671, 76362, 49978, 97755, 27341, 85268, 10550, 117515, 108397, 73712, 7893, 70950, 32702, 15996, 91233, 830, 111734, 121951, 85437, 137808, 99613, 129486, 126453, 14242, 135650, 98973, 57520, 41957, 127303, 43683, 119353, 55573, 11174, 120117, 5815, 46554, 83167, 238, 31018, 121797, 52966, 126800, 103948, 86817, 41262, 42058, 134780, 122725, 119833, 42653, 55349, 83965, 112190, 15937, 88731, 75176, 21071, 58509, 100256, 40626, 5461, 86240, 53760, 6426, 17485, 104717, 79803, 62224, 80267, 88843, 13885, 110863, 99062, 122601, 111379] not in index'"
     ]
    }
   ],
   "source": [
    "from sklearn.w2v_selection import train_test_split\n",
    "\n",
    "def bal_split(X,y):\n",
    "    # Split the data into training and testing sets (70-30 split)\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    # Split the remaining data into training and validation sets (50-50 split)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "    # Balance the VALIDATION set\n",
    "    positive_val_indices = np.where(y_val == 1)[0]\n",
    "    negative_val_indices = np.where(y_val == 0)[0]\n",
    "    np.random.seed(42)\n",
    "    positive_val_indices = np.random.choice(positive_val_indices, size=1000, replace=False)\n",
    "    negative_val_indices = np.random.choice(negative_val_indices, size=1000, replace=False)\n",
    "    X_val_balanced = np.concatenate((X_val[positive_val_indices], X_val[negative_val_indices]), axis=0)\n",
    "    y_val_balanced = np.concatenate((np.ones(1000), np.zeros(1000)))\n",
    "\n",
    "    return X_train, X_val_balanced, X_test, y_train, y_val_balanced, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Print the size and distribution of each set\n",
    "print(\"Size of training set:\", len(X_train))\n",
    "print(\"Distribution of labels in training set:\")\n",
    "print(pd.Series(y_train).value_counts())\n",
    "print()\n",
    "print(\"Size of test set:\", len(X_test))\n",
    "print(\"Distribution of labels in validation set:\")\n",
    "print(pd.Series(y_test).value_counts())\n",
    "print()\n",
    "print(\"Size of balanced val set:\", len(X_val_balanced))\n",
    "print(\"Distribution of labels in balanced test set:\")\n",
    "print(pd.Series(y_val_balanced).value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completely Naive BoW with Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_w2v import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.w2v_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Convert text data to bag of words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#Prediction\n",
    "lr_naive = LogisticRegression()\n",
    "lr_naive.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing dataset\n",
    "y_pred = lr_naive.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "#88.4% out of the box accuracy on regular data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Convert text data to bag of words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "pos_test_set = pos_test_set.toarray()\n",
    "neg_test_set = neg_test_set.toarray()\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Test the model on the testing dataset\n",
    "y_pred = lr.predict(balanced_X_test)\n",
    "\n",
    "y_pred_naive = lr_naive.predict(balanced_X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(balanced_y_test, y_pred)\n",
    "accuracy_naive = accuracy_score(balanced_y_test, y_pred_naive)\n",
    "\n",
    "\n",
    "\n",
    "#Accuracy 81% on balanced panel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    862\n",
      "0.0    862\n",
      "dtype: int64\n",
      "Accuracy: 0.810\n",
      "Naive Accuracy: 0.804\n"
     ]
    }
   ],
   "source": [
    "print(pd.Series(balanced_y_test).value_counts())\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Naive Accuracy: {accuracy_naive:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on random sample: 89.58%\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.3, random_state=42)\n",
    "accuracy = lr.score (X_test,y_test)\n",
    "print('Accuracy on random sample: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "#89% accuracy on random sample "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Boosting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-logloss:0.58410\n",
      "[1]\ttest-logloss:0.52431\n",
      "[2]\ttest-logloss:0.48875\n",
      "[3]\ttest-logloss:0.46518\n",
      "[4]\ttest-logloss:0.44845\n",
      "[5]\ttest-logloss:0.43597\n",
      "[6]\ttest-logloss:0.42539\n",
      "[7]\ttest-logloss:0.41787\n",
      "[8]\ttest-logloss:0.41111\n",
      "[9]\ttest-logloss:0.40497\n",
      "[10]\ttest-logloss:0.39932\n",
      "[11]\ttest-logloss:0.39470\n",
      "[12]\ttest-logloss:0.38928\n",
      "[13]\ttest-logloss:0.38553\n",
      "[14]\ttest-logloss:0.38167\n",
      "[15]\ttest-logloss:0.37752\n",
      "[16]\ttest-logloss:0.37365\n",
      "[17]\ttest-logloss:0.37069\n",
      "[18]\ttest-logloss:0.36773\n",
      "[19]\ttest-logloss:0.36498\n",
      "[20]\ttest-logloss:0.36281\n",
      "[21]\ttest-logloss:0.36031\n",
      "[22]\ttest-logloss:0.35754\n",
      "[23]\ttest-logloss:0.35540\n",
      "[24]\ttest-logloss:0.35408\n",
      "[25]\ttest-logloss:0.35234\n",
      "[26]\ttest-logloss:0.35038\n",
      "[27]\ttest-logloss:0.34849\n",
      "[28]\ttest-logloss:0.34648\n",
      "[29]\ttest-logloss:0.34345\n",
      "[30]\ttest-logloss:0.34244\n",
      "[31]\ttest-logloss:0.34082\n",
      "[32]\ttest-logloss:0.33863\n",
      "[33]\ttest-logloss:0.33703\n",
      "[34]\ttest-logloss:0.33516\n",
      "[35]\ttest-logloss:0.33441\n",
      "[36]\ttest-logloss:0.33348\n",
      "[37]\ttest-logloss:0.33218\n",
      "[38]\ttest-logloss:0.33061\n",
      "[39]\ttest-logloss:0.32975\n",
      "[40]\ttest-logloss:0.32885\n",
      "[41]\ttest-logloss:0.32815\n",
      "[42]\ttest-logloss:0.32669\n",
      "[43]\ttest-logloss:0.32545\n",
      "[44]\ttest-logloss:0.32419\n",
      "[45]\ttest-logloss:0.32230\n",
      "[46]\ttest-logloss:0.32121\n",
      "[47]\ttest-logloss:0.32008\n",
      "[48]\ttest-logloss:0.31933\n",
      "[49]\ttest-logloss:0.31840\n",
      "[50]\ttest-logloss:0.31706\n",
      "[51]\ttest-logloss:0.31664\n",
      "[52]\ttest-logloss:0.31617\n",
      "[53]\ttest-logloss:0.31546\n",
      "[54]\ttest-logloss:0.31421\n",
      "[55]\ttest-logloss:0.31354\n",
      "[56]\ttest-logloss:0.31270\n",
      "[57]\ttest-logloss:0.31222\n",
      "[58]\ttest-logloss:0.31159\n",
      "[59]\ttest-logloss:0.31128\n",
      "[60]\ttest-logloss:0.31044\n",
      "[61]\ttest-logloss:0.30999\n",
      "[62]\ttest-logloss:0.30944\n",
      "[63]\ttest-logloss:0.30827\n",
      "[64]\ttest-logloss:0.30736\n",
      "[65]\ttest-logloss:0.30693\n",
      "[66]\ttest-logloss:0.30649\n",
      "[67]\ttest-logloss:0.30616\n",
      "[68]\ttest-logloss:0.30597\n",
      "[69]\ttest-logloss:0.30545\n",
      "[70]\ttest-logloss:0.30467\n",
      "[71]\ttest-logloss:0.30369\n",
      "[72]\ttest-logloss:0.30301\n",
      "[73]\ttest-logloss:0.30259\n",
      "[74]\ttest-logloss:0.30229\n",
      "[75]\ttest-logloss:0.30223\n",
      "[76]\ttest-logloss:0.30199\n",
      "[77]\ttest-logloss:0.30188\n",
      "[78]\ttest-logloss:0.30151\n",
      "[79]\ttest-logloss:0.30069\n",
      "[80]\ttest-logloss:0.30005\n",
      "[81]\ttest-logloss:0.29910\n",
      "[82]\ttest-logloss:0.29877\n",
      "[83]\ttest-logloss:0.29855\n",
      "[84]\ttest-logloss:0.29804\n",
      "[85]\ttest-logloss:0.29721\n",
      "[86]\ttest-logloss:0.29652\n",
      "[87]\ttest-logloss:0.29600\n",
      "[88]\ttest-logloss:0.29578\n",
      "[89]\ttest-logloss:0.29525\n",
      "[90]\ttest-logloss:0.29459\n",
      "[91]\ttest-logloss:0.29382\n",
      "[92]\ttest-logloss:0.29341\n",
      "[93]\ttest-logloss:0.29312\n",
      "[94]\ttest-logloss:0.29239\n",
      "[95]\ttest-logloss:0.29204\n",
      "[96]\ttest-logloss:0.29158\n",
      "[97]\ttest-logloss:0.29160\n",
      "[98]\ttest-logloss:0.29166\n",
      "[99]\ttest-logloss:0.29120\n",
      "[100]\ttest-logloss:0.29090\n",
      "[101]\ttest-logloss:0.29058\n",
      "[102]\ttest-logloss:0.28972\n",
      "[103]\ttest-logloss:0.28957\n",
      "[104]\ttest-logloss:0.28980\n",
      "[105]\ttest-logloss:0.28972\n",
      "[106]\ttest-logloss:0.28954\n",
      "[107]\ttest-logloss:0.28926\n",
      "[108]\ttest-logloss:0.28909\n",
      "[109]\ttest-logloss:0.28918\n",
      "[110]\ttest-logloss:0.28862\n",
      "[111]\ttest-logloss:0.28799\n",
      "[112]\ttest-logloss:0.28761\n",
      "[113]\ttest-logloss:0.28734\n",
      "[114]\ttest-logloss:0.28697\n",
      "[115]\ttest-logloss:0.28686\n",
      "[116]\ttest-logloss:0.28668\n",
      "[117]\ttest-logloss:0.28645\n",
      "[118]\ttest-logloss:0.28610\n",
      "[119]\ttest-logloss:0.28571\n",
      "[120]\ttest-logloss:0.28560\n",
      "[121]\ttest-logloss:0.28539\n",
      "[122]\ttest-logloss:0.28520\n",
      "[123]\ttest-logloss:0.28492\n",
      "[124]\ttest-logloss:0.28484\n",
      "[125]\ttest-logloss:0.28449\n",
      "[126]\ttest-logloss:0.28395\n",
      "[127]\ttest-logloss:0.28394\n",
      "[128]\ttest-logloss:0.28339\n",
      "[129]\ttest-logloss:0.28319\n",
      "[130]\ttest-logloss:0.28283\n",
      "[131]\ttest-logloss:0.28250\n",
      "[132]\ttest-logloss:0.28210\n",
      "[133]\ttest-logloss:0.28176\n",
      "[134]\ttest-logloss:0.28178\n",
      "[135]\ttest-logloss:0.28194\n",
      "[136]\ttest-logloss:0.28166\n",
      "[137]\ttest-logloss:0.28161\n",
      "[138]\ttest-logloss:0.28142\n",
      "[139]\ttest-logloss:0.28105\n",
      "[140]\ttest-logloss:0.28072\n",
      "[141]\ttest-logloss:0.28060\n",
      "[142]\ttest-logloss:0.28040\n",
      "[143]\ttest-logloss:0.28042\n",
      "[144]\ttest-logloss:0.28024\n",
      "[145]\ttest-logloss:0.28009\n",
      "[146]\ttest-logloss:0.28001\n",
      "[147]\ttest-logloss:0.27979\n",
      "[148]\ttest-logloss:0.27975\n",
      "[149]\ttest-logloss:0.27947\n",
      "[150]\ttest-logloss:0.27901\n",
      "[151]\ttest-logloss:0.27853\n",
      "[152]\ttest-logloss:0.27830\n",
      "[153]\ttest-logloss:0.27823\n",
      "[154]\ttest-logloss:0.27796\n",
      "[155]\ttest-logloss:0.27780\n",
      "[156]\ttest-logloss:0.27753\n",
      "[157]\ttest-logloss:0.27757\n",
      "[158]\ttest-logloss:0.27753\n",
      "[159]\ttest-logloss:0.27702\n",
      "[160]\ttest-logloss:0.27679\n",
      "[161]\ttest-logloss:0.27678\n",
      "[162]\ttest-logloss:0.27666\n",
      "[163]\ttest-logloss:0.27668\n",
      "[164]\ttest-logloss:0.27657\n",
      "[165]\ttest-logloss:0.27650\n",
      "[166]\ttest-logloss:0.27618\n",
      "[167]\ttest-logloss:0.27586\n",
      "[168]\ttest-logloss:0.27575\n",
      "[169]\ttest-logloss:0.27547\n",
      "[170]\ttest-logloss:0.27532\n",
      "[171]\ttest-logloss:0.27523\n",
      "[172]\ttest-logloss:0.27523\n",
      "[173]\ttest-logloss:0.27494\n",
      "[174]\ttest-logloss:0.27496\n",
      "[175]\ttest-logloss:0.27499\n",
      "[176]\ttest-logloss:0.27497\n",
      "[177]\ttest-logloss:0.27486\n",
      "[178]\ttest-logloss:0.27469\n",
      "[179]\ttest-logloss:0.27475\n",
      "[180]\ttest-logloss:0.27442\n",
      "[181]\ttest-logloss:0.27424\n",
      "[182]\ttest-logloss:0.27402\n",
      "[183]\ttest-logloss:0.27404\n",
      "[184]\ttest-logloss:0.27392\n",
      "[185]\ttest-logloss:0.27341\n",
      "[186]\ttest-logloss:0.27349\n",
      "[187]\ttest-logloss:0.27346\n",
      "[188]\ttest-logloss:0.27343\n",
      "[189]\ttest-logloss:0.27349\n",
      "[190]\ttest-logloss:0.27333\n",
      "[191]\ttest-logloss:0.27285\n",
      "[192]\ttest-logloss:0.27297\n",
      "[193]\ttest-logloss:0.27289\n",
      "[194]\ttest-logloss:0.27243\n",
      "[195]\ttest-logloss:0.27222\n",
      "[196]\ttest-logloss:0.27212\n",
      "[197]\ttest-logloss:0.27199\n",
      "[198]\ttest-logloss:0.27202\n",
      "[199]\ttest-logloss:0.27176\n",
      "[200]\ttest-logloss:0.27183\n",
      "[201]\ttest-logloss:0.27177\n",
      "[202]\ttest-logloss:0.27178\n",
      "[203]\ttest-logloss:0.27181\n",
      "[204]\ttest-logloss:0.27166\n",
      "[205]\ttest-logloss:0.27167\n",
      "[206]\ttest-logloss:0.27177\n",
      "[207]\ttest-logloss:0.27134\n",
      "[208]\ttest-logloss:0.27128\n",
      "[209]\ttest-logloss:0.27099\n",
      "[210]\ttest-logloss:0.27102\n",
      "[211]\ttest-logloss:0.27106\n",
      "[212]\ttest-logloss:0.27044\n",
      "[213]\ttest-logloss:0.27031\n",
      "[214]\ttest-logloss:0.27022\n",
      "[215]\ttest-logloss:0.27025\n",
      "[216]\ttest-logloss:0.27046\n",
      "[217]\ttest-logloss:0.27040\n",
      "[218]\ttest-logloss:0.27016\n",
      "[219]\ttest-logloss:0.26996\n",
      "[220]\ttest-logloss:0.26964\n",
      "[221]\ttest-logloss:0.26956\n",
      "[222]\ttest-logloss:0.26966\n",
      "[223]\ttest-logloss:0.26945\n",
      "[224]\ttest-logloss:0.26941\n",
      "[225]\ttest-logloss:0.26923\n",
      "[226]\ttest-logloss:0.26910\n",
      "[227]\ttest-logloss:0.26888\n",
      "[228]\ttest-logloss:0.26873\n",
      "[229]\ttest-logloss:0.26875\n",
      "[230]\ttest-logloss:0.26882\n",
      "[231]\ttest-logloss:0.26873\n",
      "[232]\ttest-logloss:0.26840\n",
      "[233]\ttest-logloss:0.26825\n",
      "[234]\ttest-logloss:0.26802\n",
      "[235]\ttest-logloss:0.26778\n",
      "[236]\ttest-logloss:0.26793\n",
      "[237]\ttest-logloss:0.26769\n",
      "[238]\ttest-logloss:0.26778\n",
      "[239]\ttest-logloss:0.26775\n",
      "[240]\ttest-logloss:0.26764\n",
      "[241]\ttest-logloss:0.26764\n",
      "[242]\ttest-logloss:0.26750\n",
      "[243]\ttest-logloss:0.26745\n",
      "[244]\ttest-logloss:0.26741\n",
      "[245]\ttest-logloss:0.26740\n",
      "[246]\ttest-logloss:0.26743\n",
      "[247]\ttest-logloss:0.26744\n",
      "[248]\ttest-logloss:0.26754\n",
      "[249]\ttest-logloss:0.26753\n",
      "[250]\ttest-logloss:0.26755\n",
      "[251]\ttest-logloss:0.26752\n",
      "[252]\ttest-logloss:0.26761\n",
      "[253]\ttest-logloss:0.26752\n",
      "[254]\ttest-logloss:0.26758\n",
      "[255]\ttest-logloss:0.26757\n",
      "Accuracy on entire thing: 0.8895939086294417\n",
      "Accuracy on balanced part: 0.5\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Train a logistic regression model using XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {'objective': 'binary:logistic'}\n",
    "xgb_bow = xgb.train(params, dtrain, num_boost_round=1000, early_stopping_rounds=10, evals=[(dtest, 'test')])\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = xgb_bow.predict(dtest)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy on entire thing:', accuracy_score(y_test, y_pred.round()))\n",
    "\n",
    "dbalance = xgb.DMatrix(balanced_X_test,label = balanced_y_test)\n",
    "\n",
    "# Predict on the balanced test set\n",
    "y_pred_balanced = xgb_bow.predict(dbalance)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print('Accuracy on balanced part:', accuracy_score(balanced_y_test, y_pred_balanced.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          i have bought sever of the vital can dog food...\n",
       "1          product arriv label as jumbo salt peanut the ...\n",
       "2          this is a confect that has been around a few ...\n",
       "3          if you are look for the secret ingredi in rob...\n",
       "4          great taffi at a great price there was a wide...\n",
       "                                ...                        \n",
       "393912     great for sesam chicken this is a good if not...\n",
       "393913     im disappoint with the flavor the chocol note...\n",
       "393914     these star are small so you can give 10-15 of...\n",
       "393915     these are the best treat for train and reward...\n",
       "393916     i am veri satisfi product is as advertis i us...\n",
       "Name: Text, Length: 393917, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, random_state=42, stratify=y_trainval)\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5, 7],\n",
    "    'n_estimators': [200],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'gamma': [0, 0.1],\n",
    "    \n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "xgb = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "\n",
    "# Perform grid search using cross-validation on the training set\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=2, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "for i in range(len(grid_search.cv_results_['params'])):\n",
    "    print(\"Params:\", grid_search.cv_results_['params'][i])\n",
    "    print(\"Mean test score:\", grid_search.cv_results_['mean_test_score'][i])\n",
    "    print(\"Standard deviation of test score:\", grid_search.cv_results_['std_test_score'][i])\n",
    "    print(\"----------------------------\")\n",
    "\n",
    "# Evaluate the best hyperparameters on the validation set\n",
    "best_xgb_bow = grid_search.best_estimator_\n",
    "y_val_pred = best_xgb_bow.predict(X_val)\n",
    "val_acc = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "# Test the final model on the test set\n",
    "y_test_pred = best_xgb_bow.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the best hyperparameters and their score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "#Best parameters: {'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}\n",
    "#Best score: 0.8714412165183674\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'objective': 'binary:logistic', 'colsample_bytree': 1.0, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.5}\n",
    "best_xgb_bow = xgb.train(best_params, dtrain, num_boost_round=1000, early_stopping_rounds=10, evals=[(dtest, 'test')])\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred = best_xgb_bow.predict(dtest)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on entire thing: 0.8923857868020305\n",
      "Accuracy on balanced part: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "pos_test_set = pos_test_set.toarray()\n",
    "neg_test_set = neg_test_set.toarray()\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Evaluate performance\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy on entire thing:', accuracy_score(y_test, y_pred.round()))\n",
    "\n",
    "dbalance = xgb.DMatrix(balanced_X_test,label = balanced_y_test)\n",
    "\n",
    "# Predict on the balanced test set\n",
    "y_pred_balanced = xgb_bow.predict(dbalance)\n",
    "\n",
    "# Evaluate the predictions\n",
    "print('Accuracy on balanced part:', accuracy_score(balanced_y_test, y_pred_balanced.round()))\n",
    "\n",
    "#up to 89.2% on the entire thing, but just 50% on the balanced? I'm so confused"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF with BOOSTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(393917, 1000)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_tf = cleaned_data['Text']\n",
    "tf_idf = TfidfVectorizer(max_features=1000)\n",
    "tf_data = tf_idf.fit_transform(final_tf)\n",
    "tf_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = tf_data.toarray() # turning sparse matrix into dense matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp, y, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "# Define XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier on training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost classifier on test data\n",
    "accuracy = xgb_clf.score(balanced_X_test, balanced_y_test)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "#accuracy 74.83% on the initial run for a balanced panel \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on random sample: 89.11%\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp, y, test_size=0.3, random_state=42)\n",
    "accuracy = xgb_clf.score (X_test,y_test)\n",
    "print('Accuracy on random sample: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "#89% accuracy on random sample "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(temp, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, random_state=42, stratify=y_trainval)\n",
    "\n",
    "# Define a grid of hyperparameters to search over\n",
    "param_grid = {\n",
    "    #'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    #'subsample': [0.5, 0.75, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    #'gamma': [0, 0.1, 0.2],\n",
    "    #'reg_alpha': [0, 0.1, 0.5],\n",
    "    #'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "# Create an XGBoost classifier object\n",
    "xgb = XGBClassifier(objective='binary:logistic', use_label_encoder=False)\n",
    "\n",
    "# Perform grid search using cross-validation on the training set\n",
    "grid_search = GridSearchCV(xgb, param_grid, cv=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best hyperparameters on the validation set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_val_pred = best_xgb.predict(X_val)\n",
    "val_acc = best_xgb.score(y_val, y_val_pred)\n",
    "\n",
    "# Test the final model on the test set\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "test_acc = best_xgb.score(y_test, y_test_pred)\n",
    "\n",
    "# Print the best hyperparameters and their score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "\n",
    "#87.4% accuracy w 0.75 colsample by tree, max depth o f7 and 200 estimators, subsample 1, gamma 0, alpha 0, lambda 1\n",
    "#takes an entire hour \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'colsample_bytree': 0.75, 'max_depth': 7, 'n_estimators': 200}\n",
      "Best score: 0.8739259391682477\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best hyperparameters on the validation set\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_val_pred = best_xgb.predict(X_val)\n",
    "val_acc = best_xgb.score(y_val, y_val_pred)\n",
    "\n",
    "# Test the final model on the test set\n",
    "y_test_pred = best_xgb.predict(X_test)\n",
    "test_acc = best_xgb.score(y_test, y_test_pred)\n",
    "\n",
    "# Print the best hyperparameters and their score\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8387470997679815\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp, y, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "# Test the model on the testing dataset\n",
    "y_pred = best_xgb.predict(balanced_X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = np.mean(y_pred == balanced_y_test)\n",
    "\n",
    "print(accuracy)\n",
    "\n",
    "#83.8% on balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on random sample: 91.19%\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(temp, y, test_size=0.3, random_state=42)\n",
    "accuracy = best_xgb.score (X_test,y_test)\n",
    "print('Accuracy on random sample: {:.2f}%'.format(accuracy * 100))\n",
    "\n",
    "#91.19% accuracy on random sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time taken: 0.0\n",
      "[' i do not know']\n",
      "<class 'numpy.ndarray'>\n",
      "Probability of a positive review is 0.2703675627708435\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"I do not know\"]\n",
    "clean_sentence = clean_data(sentence)\n",
    "print(clean_sentence)\n",
    "sparse = tf_idf.transform(clean_sentence)\n",
    "\n",
    "sparse = sparse.toarray()\n",
    "\n",
    "prob = best_xgb.predict_proba(sparse)\n",
    "print(type(prob))\n",
    "print(f'Probability of a positive review is {prob[0][1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.3min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.4min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 1.7min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.5, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 1.9min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.8min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.0min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 2.4min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.7min\n",
      "[CV] END colsample_bytree=0.75, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 2.7min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.2min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.2min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.7min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.1, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.9min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.2, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.5; total time= 2.5min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.1min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=0.75; total time= 3.2min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.6min\n",
      "[CV] END colsample_bytree=1.0, learning_rate=0.3, max_depth=7, n_estimators=200, subsample=1.0; total time= 3.6min\n",
      "Best hyperparameters: {'colsample_bytree': 0.75, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.75}\n",
      "Accuracy: 83.65%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load pre-trained word2vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# load and preprocess data\n",
    "\n",
    "cleaned_data['tokens'] = cleaned_data['Text'].apply(lambda x: x.split())\n",
    "sentences = cleaned_data['tokens'].tolist()\n",
    "\n",
    "# convert text to numerical vectors using word embeddings\n",
    "X = np.zeros((len(sentences), model.vector_size))\n",
    "for i, sentence in enumerate(sentences):\n",
    "    vector = np.zeros(model.vector_size)\n",
    "    for word in sentence:\n",
    "        if word in model.key_to_index:\n",
    "            vector += model.get_vector(word)\n",
    "    X[i] = vector\n",
    "\n",
    "# define target variable\n",
    "Y = cleaned_data['Score'].values\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "# initialize XGBoost model\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "# define parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'colsample_bytree': [0.5, 0.75, 1.0],\n",
    "    'subsample': [0.5, 0.75, 1.0],\n",
    "    'learning_rate': [0.1,0.2,0.3],\n",
    "}\n",
    "\n",
    "# perform grid search\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=2, verbose = 2)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# get best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# train XGBoost model on training data with best hyperparameters\n",
    "xgb_model = xgb.XGBClassifier(**best_params)\n",
    "xgb_model.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the model on testing data\n",
    "accuracy = xgb_model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "#83% accuracy naively \n",
    "#83.65% with tuned hyperparameters... \n",
    "#Best hyperparameters: {'colsample_bytree': 0.75, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 200, 'subsample': 0.75}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6136\\1325288144.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Train XGBoost classifier on training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mxgb_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Evaluate XGBoost classifier on test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1469\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m             )\n\u001b[1;32m-> 1471\u001b[1;33m             train_dmatrix, evals = _wrap_evaluation_matrices(\n\u001b[0m\u001b[0;32m   1472\u001b[0m                 \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[1;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[0;32m    446\u001b[0m     \"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\n\u001b[0;32m    447\u001b[0m     way.\"\"\"\n\u001b[1;32m--> 448\u001b[1;33m     train_dmatrix = create_dmatrix(\n\u001b[0m\u001b[0;32m    449\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36m_create_dmatrix\u001b[1;34m(self, ref, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnthread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_evaluation_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingCallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvalsLog\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m         handle, feature_names, feature_types = dispatch_data_backend(\n\u001b[0m\u001b[0;32m    744\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m             \u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[0;32m    947\u001b[0m         )\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_from_numpy_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_from_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_from_numpy_array\u001b[1;34m(data, missing, nthread, feature_names, feature_types)\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[1;34m\"Expecting 2 dimensional numpy.ndarray, got: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         )\n\u001b[1;32m--> 193\u001b[1;33m     \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ensure_np_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m     \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m     args = {\n",
      "\u001b[1;32mc:\\Users\\micha\\anaconda3\\lib\\site-packages\\xgboost\\data.py\u001b[0m in \u001b[0;36m_ensure_np_dtype\u001b[1;34m(data, dtype)\u001b[0m\n\u001b[0;32m    158\u001b[0m ) -> Tuple[np.ndarray, Optional[NumpyDType]]:\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasobject\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "X_w2v = np.array(cleaned_data['encoded_text'])\n",
    "X_w2v = X_w2v.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w2v, y, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "# Define XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "\n",
    "# Train XGBoost classifier on training data\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate XGBoost classifier on test data\n",
    "accuracy = xgb_clf.score(balanced_X_test, balanced_y_test)\n",
    "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Encode documents using 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "encoded_documents = vectorizer.fit_transform(cleaned_data['Text'])\n",
    "\n",
    "\n",
    "# Define CountVectorizer with n-gram range\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Fit and transform text data\n",
    "X = vectorizer.fit_transform(cleaned_data['Text'])\n",
    "\n",
    "# Convert sparse matrix to array\n",
    "X = np.array(X.toarray())\n",
    "\n",
    "# Print feature names\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(\"C:/Users/micha/Downloads/sentiment-analysis-on-movie-reviews/train.tsv/train.tsv\", on_bad_lines='skip', sep = '\\t')\n",
    "movie_reviews = movie_reviews.drop_duplicates(subset=['SentenceId'], keep='first')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time taken: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8529,), (8529,))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Parallelization \n",
    "X_movie = clean_data(movie_reviews['Phrase'])\n",
    "y_movie = movie_reviews['Sentiment'] + 1\n",
    "\n",
    "y_movie[y_movie<4] = 0 # negative class\n",
    "y_movie[y_movie>=4] = 1 # positive class\n",
    "\n",
    "y_movie.value_counts().plot(kind='bar')\n",
    "\n",
    "X_movie = np.array(X_movie)\n",
    "X_movie.shape, y_movie.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.46511627906976744\n"
     ]
    }
   ],
   "source": [
    "X_movie = tf_idf.transform(X_movie)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_movie, y_movie, train_size = .7, test_size=0.01, random_state=42)\n",
    "\n",
    "# Calculate frequency of each class in the training set\n",
    "class_freq = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# Split testing set into two parts, one for each class\n",
    "pos_idx = np.where(y_test == 1)[0]\n",
    "neg_idx = np.where(y_test == 0)[0]\n",
    "\n",
    "pos_test_set = X_test[pos_idx]\n",
    "neg_test_set = X_test[neg_idx]\n",
    "\n",
    "pos_test_set = pos_test_set.toarray()\n",
    "neg_test_set = neg_test_set.toarray()\n",
    "\n",
    "# Combine the two testing sets to create a balanced testing set\n",
    "min_len = min(pos_test_set.shape[0], neg_test_set.shape[0])\n",
    "\n",
    "balanced_X_test = np.concatenate((pos_test_set[:min_len], neg_test_set[:min_len]))\n",
    "balanced_y_test = np.concatenate((np.ones(min_len), np.zeros(min_len)))\n",
    "\n",
    "# Shuffle the examples in the balanced testing set\n",
    "shuffle_idx = np.random.permutation(len(balanced_y_test))\n",
    "balanced_X_test = balanced_X_test[shuffle_idx]\n",
    "balanced_y_test = balanced_y_test[shuffle_idx]\n",
    "\n",
    "\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
