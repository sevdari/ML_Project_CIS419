{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "from tqdm import trange\n",
    "\n",
    "import re\n",
    "\n",
    "import time\n",
    "import torch.nn as nn\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv')\n",
    "data = data.drop_duplicates(subset = {\"UserId\",\"ProfileName\",\"Time\",\"Text\"})\n",
    "data = data[data['HelpfulnessNumerator'] <= data['HelpfulnessDenominator']]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 10000\n"
     ]
    }
   ],
   "source": [
    "X, y = data['Text'][:10000], data['Score'][:10000]\n",
    "y[y<4] = 0 # negative class\n",
    "y[y>=4] = 1 # positive class\n",
    "y = np.array(y)\n",
    "print('total:', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized_texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Time taken: 0.0\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for i, sentence in enumerate(X):\n",
    "    if i % 10000 == 0:\n",
    "        print(i, 'Time taken:', time.time() - start)\n",
    "        start = time.time()\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)  # Removing HTML tags\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]', r' ', sentence)  # Removing Punctuations\n",
    "    tokenized_texts.append(tokenizer.tokenize(' '.join(['[CLS]', sentence, '[SEP]'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(len(tokenized_texts)):\n",
    "    if len(tokenized_texts[i]) < 128:\n",
    "        input_ids.append(np.pad(np.array(tokenizer.convert_tokens_to_ids(tokenized_texts[i])), (0, 128 - len(tokenized_texts[i]))))\n",
    "        attention_masks.append([float(j>0) for j in input_ids[count]])\n",
    "        labels.append(y[count])\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels,\n",
    "                                                            random_state=42, test_size=0.2)\n",
    "train_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=42, test_size=0.2)\n",
    "train_masks, val_masks, _, _ = train_test_split(train_masks, train_inputs,\n",
    "                                             random_state=42, test_size=0.2)\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels,\n",
    "                                                            random_state=42, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stratify(data, length):\n",
    "    positive = np.where(data == 1)[0]\n",
    "    negative = np.where(data == 0)[0]\n",
    "    draw_p = positive[np.random.permutation(len(positive))[:length]]\n",
    "    draw_n = negative[np.random.permutation(len(negative))[:length]]\n",
    "    draw = np.hstack([draw_p, draw_n])\n",
    "    np.random.shuffle(draw)\n",
    "    return draw[:int(length * 1.5)]\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "draw_train = stratify(train_labels, 2000)\n",
    "draw_val = stratify(val_labels, 200)\n",
    "draw_test = stratify(test_labels, 200)\n",
    "\n",
    "train_inputs = [train_inputs[i] for i in draw_train]\n",
    "train_masks = [train_masks[i] for i in draw_train]\n",
    "train_labels = [train_labels[i] for i in draw_train]\n",
    "\n",
    "val_inputs = [val_inputs[i] for i in draw_val]\n",
    "val_masks = [val_masks[i] for i in draw_val]\n",
    "val_labels = [val_labels[i] for i in draw_val]\n",
    "\n",
    "test_inputs = [test_inputs[i] for i in draw_test]\n",
    "test_masks = [test_masks[i] for i in draw_test]\n",
    "test_labels = [test_labels[i] for i in draw_test]\n",
    "\n",
    "print(np.sum(train_labels) / len(train_labels))\n",
    "print(np.sum(val_labels) / len(val_labels))\n",
    "print(np.sum(test_labels) / len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = torch.tensor(train_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "test_masks = torch.tensor(test_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an iterator of our data with torch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, sampler=RandomSampler(train_data))\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size=32, sampler=SequentialSampler(val_data))\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, sampler=SequentialSampler(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, eps=1e-8, warmup=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_set = []\n",
    "# Number of training epochs\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "    ## TRAINING\n",
    "    model.train()\n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    # Train the data for one epoch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to GPU\n",
    "        if step % 100 == 0:\n",
    "            print(step, 'time:', time.time() - start)\n",
    "            start = time.time()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        train_loss_set.append(loss.item())\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "\n",
    "    ## VALIDATION\n",
    "\n",
    "    # Put model in evaluation mode\n",
    "    model.eval()\n",
    "    # Tracking variables\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in val_dataloader:\n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        print(logits)\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += len(b_labels)\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "## Prediction on test set<br>\n",
    "# Put model in evaluation mode<br>\n",
    "model.eval()<br>\n",
    "# Tracking variables<br>\n",
    "predictions, true_labels = [], []<br>\n",
    "# Predict<br>\n",
    "for batch in test_dataloader:<br>\n",
    "    # Add batch to GPU<br>\n",
    "    batch = tuple(t.to(device) for t in batch)<br>\n",
    "    # Unpack the inputs from our dataloader<br>\n",
    "    b_input_ids, b_input_mask, b_labels = batch<br>\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up prediction<br>\n",
    "    with torch.no_grad():<br>\n",
    "        # Forward pass, calculate logit predictions<br>\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)<br>\n",
    "    # Move logits and labels to CPU<br>\n",
    "    logits = logits.detach().cpu().numpy()<br>\n",
    "    label_ids = b_labels.to('cpu').numpy()<br>\n",
    "    # Store predictions and true labels<br>\n",
    "    predictions.append(logits)<br>\n",
    "    true_labels.append(label_ids)<br>\n",
    "# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset<br>\n",
    "flat_predictions = [item for sublist in predictions for item in sublist]<br>\n",
    "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()<br>\n",
    "flat_true_labels = [item for sublist in true_labels for item in sublist]<br>\n",
    "print('Classification accuracy using BERT Fine Tuning: ', 1.0 * flat_true_labels / (1.0 * flat_predictions))<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
